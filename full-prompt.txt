# ─────────────────────────────────────────────────────────────────────────────
# Project: local-first-voice-chatbot
# What you get:
# - docker-compose.yml (FastAPI gateway + Piper TTS)
# - server/app (FastAPI: STT via faster-whisper, LLM via llama.cpp or OpenAI, TTS proxy)
# - web/ (Minimal HTML/JS client with push-to-talk, WAV encoder)
# - scripts/ (helpers to fetch models)
# - .env.example (toggle local vs cloud)
# ─────────────────────────────────────────────────────────────────────────────

# =========================
# docker-compose.yml
# =========================
version: "3.9"
services:
  api:
    build: ./server
    container_name: voicebot_api
    ports:
      - "8000:8000"
    env_file:
      - .env
    volumes:
      - ./models:/models
      - ./data:/data
      - ./server/app:/app
    depends_on:
      - piper
    command: ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "1"]

  piper:
    image: ghcr.io/rhasspy/piper:latest
    container_name: piper_tts
    volumes:
      - ./models/piper:/piper
    command: ["--model", "/piper/en_US-amy-low.onnx", "--output_raw", "--sentence_silence", "0.2"]
    # Piper listens on stdin/stdout in this image. We'll call it via subprocess from API.


# =========================
# .env.example (copy to .env and edit)
# =========================
# Toggle engines
USE_LOCAL_LLM=true
USE_OPENAI_FALLBACK=false
USE_LOCAL_STT=true
USE_CLOUD_STT=false

# LLM: local (llama.cpp) model
LLM_MODEL_PATH=/models/llm/qwen2-7b-instruct-q4_k_m.gguf
LLM_CTX=4096
LLM_GPU_LAYERS=0

# OpenAI fallback (optional)
OPENAI_API_KEY=
OPENAI_MODEL=gpt-4o-mini

# Faster-Whisper (STT)
STT_MODEL_SIZE=small
STT_DEVICE=auto           # "cpu" or "cuda"
STT_COMPUTE_TYPE=int8     # or float16 for GPU

# Piper voice model
PIPER_MODEL=en_US-amy-low.onnx

# CORS for web client
CORS_ORIGINS=http://localhost:5173,http://localhost:8080,http://127.0.0.1:8000


# =========================
# server/Dockerfile
# =========================
FROM python:3.11-slim

RUN apt-get update && apt-get install -y \
    build-essential git ffmpeg curl && \
    rm -rf /var/lib/apt/lists/*

WORKDIR /app
COPY app/requirements.txt /app/requirements.txt
RUN pip install --no-cache-dir -r /app/requirements.txt

COPY app /app

# Models directory (mounted from host)
VOLUME ["/models", "/data"]

EXPOSE 8000


# =========================
# server/app/requirements.txt
# =========================
fastapi==0.115.0
uvicorn==0.30.6
pydantic==2.8.2
python-dotenv==1.0.1
starlette==0.38.4
httpx==0.27.0
websockets==12.0
numpy==2.0.1
soundfile==0.12.1
scipy==1.14.0
# STT
faster-whisper==1.0.3
# LLM local
llama-cpp-python==0.2.90
# OpenAI fallback (optional)
openai==1.43.0
# Embeddings (optional future RAG)
sentence-transformers==3.0.1


# =========================
# server/app/main.py
# =========================
import io
import json
import os
import subprocess
import wave
from typing import Optional

import numpy as np
import soundfile as sf
from dotenv import load_dotenv
from fastapi import FastAPI, UploadFile, File, Form
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse, JSONResponse, HTMLResponse

from app.stt import transcribe_wav
from app.textgen import generate_reply
from app.tts import synth_stream

load_dotenv()

app = FastAPI(title="Local-First Voice Chatbot")

# CORS
origins = os.getenv("CORS_ORIGINS", "*").split(",")
app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/")
def root():
    return {"ok": True, "service": "voicebot_api"}


@app.post("/transcribe")
async def transcribe(file: UploadFile = File(...)):
    # Expecting WAV (PCM16, 16kHz mono) from the web client
    data = await file.read()
    audio, sr = sf.read(io.BytesIO(data), dtype="float32")
    if sr != 16000:
        # naive resample (optional: use librosa or scipy.signal.resample)
        import scipy.signal as sps
        num = int(len(audio) * 16000 / sr)
        audio = sps.resample(audio, num)
        sr = 16000
    text, info = transcribe_wav(audio, sr)
    return {"text": text, "info": info}


@app.post("/chat")
async def chat(text: str = Form(...), history: Optional[str] = Form(None)):
    try:
        history_json = json.loads(history) if history else []
    except Exception:
        history_json = []
    reply = generate_reply(text, history_json)
    return {"reply": reply}


@app.post("/tts")
async def tts(text: str = Form(...)):
    def gen():
        # Stream raw 16-bit little-endian PCM from Piper, we wrap as WAV on the fly
        with wave.open(io.BytesIO(), 'wb') as dummy:
            pass
        yield from synth_stream(text)

    # Produce a WAV header + PCM16 stream
    sample_rate = 22050  # Piper default output

    def wav_stream():
        # Write WAV header first
        # We'll write an infinite/unknown length stream: set data size to 0xFFFFFFFF
        header = _wav_header(sample_rate)
        yield header
        # Then raw PCM chunks from Piper
        for chunk in gen():
            yield chunk

    return StreamingResponse(wav_stream(), media_type="audio/wav")


def _wav_header(sr: int) -> bytes:
    # Minimal WAV header for streaming (data size set to 0xFFFFFFFF)
    import struct
    num_channels = 1
    bits_per_sample = 16
    byte_rate = sr * num_channels * bits_per_sample // 8
    block_align = num_channels * bits_per_sample // 8
    data_size = 0xFFFFFFFF
    return (
        b"RIFF" + struct.pack('<I', 36 + data_size) + b"WAVE"
        + b"fmt " + struct.pack('<IHHIIHH', 16, 1, num_channels, sr, byte_rate, block_align, bits_per_sample)
        + b"data" + struct.pack('<I', data_size)
    )


# Simple test page
@app.get("/demo")
async def demo():
    with open("/app/static/index.html", "r", encoding="utf-8") as f:
        return HTMLResponse(f.read())


# =========================
# server/app/stt.py
# =========================
import os
from faster_whisper import WhisperModel

_STT_MODEL = None


def _load_model():
    global _STT_MODEL
    if _STT_MODEL is None:
        size = os.getenv("STT_MODEL_SIZE", "small")
        device = os.getenv("STT_DEVICE", "auto")
        compute_type = os.getenv("STT_COMPUTE_TYPE", "int8")
        _STT_MODEL = WhisperModel(size, device=device, compute_type=compute_type)
    return _STT_MODEL


def transcribe_wav(audio_float32, sr):
    model = _load_model()
    segments, info = model.transcribe(audio_float32, language="en", vad_filter=True)
    text = " ".join(seg.text.strip() for seg in segments)
    return text.strip(), {"language": info.language, "duration": info.duration}


# =========================
# server/app/textgen.py
# =========================
import os
from typing import List, Dict

USE_LOCAL = os.getenv("USE_LOCAL_LLM", "true").lower() == "true"
USE_OPENAI = os.getenv("USE_OPENAI_FALLBACK", "false").lower() == "true"

# Local LLM via llama.cpp
_llm = None


def _local_llm():
    global _llm
    if _llm is None:
        from llama_cpp import Llama
        model_path = os.getenv("LLM_MODEL_PATH", "/models/llm/model.gguf")
        ctx = int(os.getenv("LLM_CTX", "4096"))
        gpu_layers = int(os.getenv("LLM_GPU_LAYERS", "0"))
        _llm = Llama(model_path=model_path, n_ctx=ctx, n_gpu_layers=gpu_layers, seed=42, verbose=False)
    return _llm


def _openai_client():
    from openai import OpenAI
    return OpenAI(api_key=os.getenv("OPENAI_API_KEY"))


def generate_reply(user_text: str, history: List[Dict]):
    system = (
        "You are a concise, helpful voice assistant."
        " Keep answers short and speakable."
    )

    messages = [{"role": "system", "content": system}] + history + [
        {"role": "user", "content": user_text}
    ]

    if USE_LOCAL:
        llm = _local_llm()
        prompt = _to_chatml(messages)
        out = llm(prompt, max_tokens=256, temperature=0.3, stop=["<|eot_id|>", "</s>"])
        text = out["choices"][0]["text"].strip()
        return text

    if USE_OPENAI:
        client = _openai_client()
        model = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
        resp = client.chat.completions.create(model=model, messages=messages, temperature=0.3, max_tokens=256)
        return resp.choices[0].message.content

    # Fallback: echo
    return f"You said: {user_text}"


def _to_chatml(messages: List[Dict]) -> str:
    # Simple ChatML-ish for Llama 3 / Qwen instruct
    parts = []
    for m in messages:
        role = m["role"]
        content = m["content"]
        if role == "system":
            parts.append(f"<|im_start|>system\n{content}<|im_end|>")
        elif role == "user":
            parts.append(f"<|im_start|>user\n{content}<|im_end|>")
        else:
            parts.append(f"<|im_start|>assistant\n{content}<|im_end|>")
    parts.append("<|im_start|>assistant\n")
    return "\n".join(parts)


# =========================
# server/app/tts.py
# =========================
import os
import subprocess
import shlex

PIPER_BIN = "piper"
VOICE_PATH = "/piper/en_US-amy-low.onnx"  # Inside piper container


def synth_stream(text: str):
    # Call Piper via docker exec for simplicity (low dependency surface)
    # It outputs raw 16-bit PCM at 22050 Hz to stdout
    container = os.getenv("PIPER_CONTAINER", "piper_tts")
    model_path = f"/piper/{os.getenv('PIPER_MODEL', 'en_US-amy-low.onnx')}"
    cmd = f"docker exec {shlex.quote(container)} piper --model {shlex.quote(model_path)} --output_raw"
    proc = subprocess.Popen(shlex.split(cmd), stdin=subprocess.PIPE, stdout=subprocess.PIPE)
    # Write text
    assert proc.stdin is not None and proc.stdout is not None
    proc.stdin.write(text.encode("utf-8"))
    proc.stdin.close()

    # Stream PCM chunks
    while True:
        chunk = proc.stdout.read(4096)
        if not chunk:
            break
        yield chunk
    proc.wait()


# =========================
# server/app/static/index.html (served at GET /demo)
# =========================
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Local Voice Chatbot Demo</title>
  <style>
    body { font-family: system-ui, sans-serif; margin: 2rem; }
    .row { display:flex; gap:1rem; align-items:center; }
    button { padding: .6rem 1rem; font-weight:600; }
    textarea { width:100%; height:6rem; }
    audio { width: 100%; margin-top: .5rem; }
  </style>
</head>
<body>
  <h1>Local Voice Chatbot (Push-to-talk MVP)</h1>
  <div class="row">
    <button id="recBtn">🎙️ Hold to speak</button>
    <span id="status">idle</span>
  </div>

  <h3>Transcript</h3>
  <textarea id="transcript" readonly></textarea>

  <h3>Assistant</h3>
  <textarea id="reply" readonly></textarea>
  <audio id="player" controls></audio>

  <script>
    const statusEl = document.getElementById('status');
    const recBtn = document.getElementById('recBtn');
    const transcriptEl = document.getElementById('transcript');
    const replyEl = document.getElementById('reply');
    const player = document.getElementById('player');

    let mediaStream, audioCtx, processor, source;

    async function startAudio() {
      mediaStream = await navigator.mediaDevices.getUserMedia({ audio: true });
      audioCtx = new (window.AudioContext || window.webkitAudioContext)({ sampleRate: 16000 });
      source = audioCtx.createMediaStreamSource(mediaStream);
      processor = audioCtx.createScriptProcessor(4096, 1, 1);
      const chunks = [];
      processor.onaudioprocess = (e) => {
        const input = e.inputBuffer.getChannelData(0);
        chunks.push(new Float32Array(input));
      };
      processor.onended = () => {};
      source.connect(processor);
      processor.connect(audioCtx.destination);
      return {
        stopAndGetWav: async () => {
          processor.disconnect();
          source.disconnect();
          audioCtx.close();
          const wav = floatTo16BitWav(concatFloat32(chunks), 16000);
          return new Blob([wav], { type: 'audio/wav' });
        }
      };
    }

    function concatFloat32(chunks) {
      let length = 0; chunks.forEach(c => length += c.length);
      const out = new Float32Array(length);
      let o = 0; for (const c of chunks) { out.set(c, o); o += c.length; }
      return out;
    }

    function floatTo16BitWav(float32Array, sampleRate) {
      const buffer = new ArrayBuffer(44 + float32Array.length * 2);
      const view = new DataView(buffer);
      const writeString = (off, s) => { for (let i=0;i<s.length;i++) view.setUint8(off+i, s.charCodeAt(i)); };
      writeString(0, 'RIFF');
      view.setUint32(4, 36 + float32Array.length * 2, true);
      writeString(8, 'WAVE');
      writeString(12, 'fmt ');
      view.setUint32(16, 16, true); // PCM chunk size
      view.setUint16(20, 1, true);  // PCM format
      view.setUint16(22, 1, true);  // mono
      view.setUint32(24, sampleRate, true);
      view.setUint32(28, sampleRate * 2, true); // byte rate
      view.setUint16(32, 2, true); // block align
      view.setUint16(34, 16, true); // bits per sample
      writeString(36, 'data');
      view.setUint32(40, float32Array.length * 2, true);
      // write samples
      let offset = 44;
      for (let i=0; i<float32Array.length; i++, offset+=2) {
        let s = Math.max(-1, Math.min(1, float32Array[i]));
        view.setInt16(offset, s < 0 ? s * 0x8000 : s * 0x7FFF, true);
      }
      return buffer;
    }

    recBtn.addEventListener('mousedown', async () => {
      statusEl.textContent = 'recording…';
      recBtn.disabled = true;
      window._rec = await startAudio();
    });
    recBtn.addEventListener('mouseup', async () => {
      statusEl.textContent = 'processing…';
      const wav = await window._rec.stopAndGetWav();
      await handleTurn(wav);
      recBtn.disabled = false;
      statusEl.textContent = 'idle';
    });

    async function handleTurn(wavBlob) {
      // 1) STT
      const form1 = new FormData();
      form1.append('file', wavBlob, 'audio.wav');
      const stt = await fetch('/transcribe', { method:'POST', body: form1 }).then(r => r.json());
      transcriptEl.value = stt.text || '';

      // 2) Chat
      const form2 = new FormData();
      form2.append('text', stt.text || '');
      form2.append('history', JSON.stringify([]));
      const ch = await fetch('/chat', { method:'POST', body: form2 }).then(r => r.json());
      replyEl.value = ch.reply || '';

      // 3) TTS
      const form3 = new FormData();
      form3.append('text', ch.reply || '');
      const ttsResp = await fetch('/tts', { method:'POST', body: form3 });
      const buf = await ttsResp.arrayBuffer();
      const blob = new Blob([buf], { type: 'audio/wav' });
      player.src = URL.createObjectURL(blob);
      await player.play();
    }
  </script>
</body>
</html>


# =========================
# scripts/get_models.sh
# =========================
#!/usr/bin/env bash
set -euo pipefail
mkdir -p models/llm models/piper

# Example GGUF (Qwen2 7B Instruct Q4_K_M) – change if you prefer Llama 3.1 8B
if [ ! -f models/llm/qwen2-7b-instruct-q4_k_m.gguf ]; then
  echo "Download a GGUF to models/llm and set LLM_MODEL_PATH in .env"
  echo "Example (manual):"
  echo "  wget -O models/llm/qwen2-7b-instruct-q4_k_m.gguf https://huggingface.co/Qwen/Qwen2-7B-Instruct-GGUF/resolve/main/qwen2-7b-instruct-q4_k_m.gguf"
fi

# Piper voice (Amy, US English – light)
if [ ! -f models/piper/en_US-amy-low.onnx ]; then
  echo "Fetching Piper voice en_US-amy-low…"
  curl -L -o models/piper/en_US-amy-low.onnx https://github.com/rhasspy/piper/releases/download/v1.2.0/en_US-amy-low.onnx
  curl -L -o models/piper/en_US-amy-low.onnx.json https://github.com/rhasspy/piper/releases/download/v1.2.0/en_US-amy-low.onnx.json
fi


echo "Done. Edit .env then run: docker compose up --build"


# =========================
# README.md (quick start)
# =========================
# Local‑first Voice Chatbot (MVP)

## Prereqs
- Docker & Docker Compose
- ~8–10 GB free disk for models (depending on LLM)

## Setup
```bash
cp .env.example .env
bash scripts/get_models.sh   # downloads Piper voice & shows how to fetch GGUF
```

Edit `.env`:
- Set `LLM_MODEL_PATH` to your GGUF file.
- Optionally set `OPENAI_API_KEY` and `USE_OPENAI_FALLBACK=true`.

## Run
```bash
docker compose up --build
```
Open the demo page:
```
http://localhost:8000/demo
```

## Notes
- This MVP uses **push-to-talk**. For streaming/barge-in, extend `/transcribe` to WebSocket and add VAD.
- STT: `faster-whisper` (size via `STT_MODEL_SIZE` in .env).
- LLM: local with `llama-cpp-python`; fallback to OpenAI when enabled.
- TTS: Piper in a sidecar; the API calls it via `docker exec` for simplicity.

## Next steps
- Replace push-to-talk with WebSocket streaming + VAD (Silero).
- Swap Piper with XTTS-v2 (GPU) for cloned voices, or wire to ElevenLabs/PlayHT.
- Add RAG (SQLite + FAISS) and long-term memory.
- Persist transcripts in /data and build “Autobiography Mode”.

